{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation library\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation données\n",
    "tweet_now = pd.read_csv(\"data_chatgpt_now.csv\",  encoding = \"utf-8\")\n",
    "tweet_before = pd.read_csv(\"data_chatgpt_before.csv\", encoding = \"utf-8\")\n",
    "\n",
    "# Suppresion des valeurs manquantes\n",
    "tweet_now = tweet_now.dropna()\n",
    "tweet_now = tweet_now.reset_index(drop=True)\n",
    "\n",
    "tweet_before = tweet_before.dropna()\n",
    "tweet_before = tweet_before.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la liste des tweets\n",
    "list_tweet_before = []\n",
    "for i in range(len(tweet_before)) :\n",
    "   list_tweet_before.append(tweet_before.iloc[i, 0].split())\n",
    "\n",
    "list_tweet_now = []\n",
    "for i in range(len(tweet_now)) :\n",
    "   list_tweet_now.append(tweet_now.iloc[i, 0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group before :  (0, '0.037*\"ia\" + 0.025*\"réponse\" + 0.024*\"openai\" + 0.023*\"nouveau\" + 0.014*\"écrire\"')\n",
      "Group before :  (1, '0.034*\"aller\" + 0.012*\"très\" + 0.010*\"information\" + 0.010*\"là\" + 0.010*\"googl\"')\n",
      "Group before :  (2, '0.040*\"avoir\" + 0.033*\"faire\" + 0.025*\"tout\" + 0.024*\"pouvoir\" + 0.019*\"plus\"')\n",
      "Group now :  (0, '0.034*\"avoir\" + 0.024*\"faire\" + 0.014*\"pouvoir\" + 0.014*\"outil\" + 0.014*\"ia\"')\n",
      "Group now :  (1, '0.026*\"openai\" + 0.022*\"plus\" + 0.021*\"googl\" + 0.019*\"via\" + 0.018*\"ia\"')\n",
      "Group now :  (2, '0.029*\"intelligence\" + 0.028*\"artificiel\" + 0.021*\"demander\" + 0.013*\"aussi\" + 0.011*\"quel\"')\n"
     ]
    }
   ],
   "source": [
    "# Pré-traitement des données\n",
    "dictionary_before = corpora.Dictionary(list_tweet_before)\n",
    "dictionary_now = corpora.Dictionary(list_tweet_now)\n",
    "\n",
    "corpus_before = [dictionary_before.doc2bow(text) for text in list_tweet_before]\n",
    "corpus_now = [dictionary_now.doc2bow(text) for text in list_tweet_now]\n",
    "\n",
    "# Entraînement du modèle LDA\n",
    "lda_model_before = LdaModel(corpus=corpus_before, id2word=dictionary_before, num_topics=3, random_state=100,\n",
    "                     update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
    "lda_model_now = LdaModel(corpus=corpus_now, id2word=dictionary_now, num_topics=3, random_state=100,\n",
    "                     update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
    "\n",
    "\n",
    "# Afficher les thèmes générés par le modèle LDA\n",
    "topics_before = lda_model_before.print_topics(num_words=5)\n",
    "for topic in topics_before:\n",
    "    print(\"Group before : \", topic)\n",
    "topics_now = lda_model_now.print_topics(num_words=5)\n",
    "for topic in topics_now:\n",
    "    print(\"Group now : \", topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f0194299e911b6a22f0cd3d1c9a66c991d39f48b249be23f24104e40900e329"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
