{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snscrape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n\u001b[1;32m      8\u001b[0m \u001b[39m#nltk.download('punkt')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m#nltk.download('wordnet')\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#nltk.download('stopwords')\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msnscrape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtwitter\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msntwitter\u001b[39;00m \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snscrape'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'fr_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m text \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39misalnum()]\n\u001b[1;32m     28\u001b[0m \u001b[39m### LEMMATISATION\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mfr_core_news_md\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatisation\u001b[39m(text):\n\u001b[1;32m     31\u001b[0m     lem \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/anaconda/lib/python3.9/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     37\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     38\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     52\u001b[0m         name, vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig\n\u001b[1;32m     53\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/lib/python3.9/site-packages/spacy/util.py:427\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    426\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'fr_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "\n",
    "### NETTOYAGE REGEX\n",
    "def nettoyage_regex(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Enlève les urls\n",
    "    text = re.sub(r'@\\S+', '', text)  # Enlève les nom d'utilisateurs\n",
    "    text = re.sub(r'#\\S+', '', text)  # Enlève les hashtags\n",
    "    \n",
    "    return text\n",
    "\n",
    "### TOKENISATION\n",
    "def tokenisation(text):\n",
    "    tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "    tweet_tokens = [tokenizer.tokenize(text)]\n",
    "    return tweet_tokens[0]\n",
    "\n",
    "### STOP WORDS\n",
    "stop_words = stopwords.words('french')\n",
    "stop_words.append(\"chatgpt\")\n",
    "stop_words.append(\"chat\")\n",
    "stop_words.append(\"gpt\")\n",
    "stop_words = set(stop_words)\n",
    "def stop_words_function(text):\n",
    "    return [w.lower() for w in text if not w.lower() in stop_words]\n",
    "\n",
    "## SUPPRESSION PONCTUATION\n",
    "def suppr_ponct(text):\n",
    "    return [token for token in text if token.isalnum()]\n",
    "\n",
    "### LEMMATISATION\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "def lemmatisation(text):\n",
    "    lem = []\n",
    "    for i in range(len(text)):\n",
    "        doc = nlp(text[i])\n",
    "        for token in doc:\n",
    "            lem.append(token.lemma_)\n",
    "    return lem\n",
    "\n",
    "def nettoyage(text):\n",
    "    text = nettoyage_regex(text)\n",
    "    text = tokenisation(text)\n",
    "    text = stop_words_function(text)\n",
    "    text = suppr_ponct(text)\n",
    "    text = lemmatisation(text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping\n",
    "maxTweets = 15000\n",
    "\n",
    "tweets_list = [] \n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list \n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query=\"chatgpt lang:fr since:2022-11-15 until:2022-12-15\").get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append(' '.join(nettoyage([tweet.content][0])))\n",
    "tweets_df_before = pd.DataFrame(tweets_list, columns=['Tweet'])\n",
    "\n",
    "\n",
    "tweets_list = [] \n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list \n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query=\"chatgpt lang:fr since:2023-01-10 until:2023-02-10\").get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append(' '.join(nettoyage([tweet.content][0])))\n",
    "tweets_df_now = pd.DataFrame(tweets_list, columns=['Tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df_now.to_csv(\"data_chatgpt_now.csv\", index = False)\n",
    "tweets_df_before.to_csv(\"data_chatgpt_before.csv\", index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f0194299e911b6a22f0cd3d1c9a66c991d39f48b249be23f24104e40900e329"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
